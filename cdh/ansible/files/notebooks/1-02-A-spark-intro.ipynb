{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Introduction\n",
    "\n",
    "In this chapter we will cover the following topics:\n",
    "\n",
    "- What is Spark?\n",
    "- Spark Shell\n",
    "- Spark & Jupyter Notebook\n",
    "- Spark execution\n",
    "- Spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Spark?\n",
    "\n",
    " - Spark is a second-generation Big Data platform\n",
    " - The first generation was based on Map-Reduce. Powerful, but cumbersome to use with technical limitations.\n",
    " - Spark uses a new architecture and leverages functional programming: these combine to make things much easier to implement and perform better than earlier frameworks.\n",
    " - Emerged from the AMPLab at UC Berkeley, at the same time as Mesos.\n",
    " - Spark is now an Apache project.\n",
    " - It's now bundled with Cloudera. You might need to add another parcel for Spark2\n",
    " - Evolving very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark advantages\n",
    "\n",
    " - Distributed processing and cluster computing\n",
    "     - Application processes are distributed across a cluster of worker nodes\n",
    "     - Data is also distributes on the worker nodes => run computation where the data lives\n",
    " - Works with distributed storage\n",
    "     - supports data locally\n",
    " - Data in memory\n",
    "     - configurable persistence for efficient iteration\n",
    " - High level programming framework\n",
    "     - programmers can focus on logic, not plumbing\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark language support\n",
    "\n",
    "Spark is internally written in Scala (a functional programming language which runs in the JVM) but supports a programming interface in the following languages:\n",
    "\n",
    "- Scala\n",
    "- Python\n",
    "- Java\n",
    "- R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with Spark\n",
    "\n",
    "There are several options to work with Spark.\n",
    "\n",
    "- Spark shell (a REPL in python or scala)\n",
    "- Jupyter notebooks\n",
    "- Spark applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Shell for Python\n",
    "\n",
    "Spark supports a shell, for interactive work. To launch this use the `pyspark` command, which starts a python REPL connected to Spark:\n",
    "\n",
    "```\n",
    "Python 3.5.3 | packaged by conda-forge | (default, Jan 24 2017, 06:45:37)\n",
    "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.54)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.5.3 (default, Jan 24 2017 06:45:37)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    ">>> sc\n",
    "<pyspark.context.SparkContext object at 0x103ba6d90>\n",
    ">>> sqlContext\n",
    "<pyspark.sql.context.SQLContext object at 0x103d7a790>\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x103d7a590>\n",
    ">>> exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Shell for Scala\n",
    "\n",
    "Spark supports a shell, for interactive work. To launch this use the `spark-shell` command, which starts a scala REPL connected to Spark:\n",
    "\n",
    "```\n",
    "% spark-shell \n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Spark context Web UI available at http://192.168.1.114:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1487929648200).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> \n",
    "\n",
    "scala> sc\n",
    "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@37b1218\n",
    "\n",
    "scala> spark\n",
    "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@88e85ascala>sc\n",
    "\n",
    "scala> exit \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark & Jupyter Notebook\n",
    "\n",
    "Jupyter was formerly part of the iPython project, but is now standalone. It's an excellent way to interactively work with Spark. To start it:\n",
    "\n",
    "    jupyter notebook\n",
    "This will start a notebook server and will direct your browser to it. Unlike the Spark shell, by default notebooks aren't connected to Spark. The most convenient way to connect is using the `findspark` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# This will search in standard locations, but setting SPARK_HOME might be required.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark execution\n",
    "\n",
    "Sparks execution environment is divided into the following components:\n",
    "\n",
    "- Driver\n",
    "    - Launches applications from outside or inside a cluster.\n",
    "- Executors\n",
    "    - Separate execution engines or containers on the worker nodes of a cluster.\n",
    "    - Tasks (unit of work) are run within the executors.\n",
    "- Cluster manager\n",
    "    - Allocates computing resources (CPU/Memory) in the distributed system the Spark application is run on.\n",
    "    - Examples are Yarn, Mesos, Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark execution model](images/spark-cluster.png \"Spark execution model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SparkContext\n",
    "\n",
    "Main entrypoint for Spark applications and a handle to the execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In [2]: sc.\n",
    "sc.PACKAGE_EXTENSIONS    sc.broadcast             sc.environment           sc.pickleFile\n",
    "sc.setCheckpointDir      sc.startTime             sc.accumulator           sc.cancelAllJobs\n",
    "sc.getLocalProperty      sc.profiler_collector    sc.setJobGroup           sc.statusTracker\n",
    "sc.addFile               sc.cancelJobGroup        sc.hadoopFile            sc.pythonExec\n",
    "sc.setLocalProperty      sc.stop                  sc.addPyFile             sc.clearFiles\n",
    "sc.hadoopRDD             sc.pythonVer             sc.setLogLevel           sc.textFile\n",
    "sc.appName               sc.defaultMinPartitions  sc.master                sc.range\n",
    "sc.setSystemProperty     sc.union                 sc.applicationId         sc.defaultParallelism\n",
    "sc.newAPIHadoopFile      sc.runJob                sc.show_profiles         sc.version\n",
    "sc.binaryFiles           sc.dump_profiles         sc.newAPIHadoopRDD       sc.sequenceFile\n",
    "sc.sparkHome             sc.wholeTextFiles        sc.binaryRecords         sc.emptyRDD\n",
    "sc.parallelize           sc.serializer            sc.sparkUser       \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
