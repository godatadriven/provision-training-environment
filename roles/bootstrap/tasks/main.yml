- name: install packages
  become: true
  become_user: root
  apt: name={{item}} state=installed #update_cache=yes
  with_items:
    # - python3-dev
    # - python3-pip
    - build-essential
    - libfreetype6-dev
    - blt-dev
    - pkg-config
    # - llvm-3.8-dev
    - libssl1.0.0
    - sudo

- name: Add aliases to file
  lineinfile: dest="~/.bashrc" line="alias ll='ls -Alh'"

- name: place requirements
  template: src=requirements.txt dest=/tmp

# - name: remove notebooks tmp folder
#   become: true
#   become_user: root
#   file:
#     state: absent
#     path: "{{ notebooks_tmp_folder }}"

# - name: Creates notebooks folder directory
#   file:
#       path: "{{ notebooks_tmp_folder }}"
#       state: directory

- name: remove notebooks folder
  become: true
  become_user: root
  file:
    state: absent
    path: "{{ notebooks_folder }}"

- name: Creates notebooks folder directory
  file:
      path: "{{ item }}"
      state: directory
  with_items:
    - "{{ notebooks_folder }}"
    - "{{ notebooks_folder }}/data"
    - "{{ notebooks_tmp_folder }}/notebooks"
    - "{{ notebooks_tmp_folder }}/answers"

- name: Check that the main user exists
  stat:
    path: /home/{{ deployer }}
  register: user_home

- name: Make sure we have a 'wheel' group
  group:
    name: wheel
    state: present
  when: user_home.stat.exists == False

- name: Allow 'wheel' group to have passwordless sudo
  lineinfile:
    dest: /etc/sudoers
    state: present
    regexp: '^%wheel'
    line: '%wheel ALL=(ALL) NOPASSWD: ALL'
  when: user_home.stat.exists == False

- name: Add sudoers deployer to wheel group
  user: name="{{ deployer }}" groups=wheel append=yes state=present createhome=yes
  when: user_home.stat.exists == False

- name: place notebooks
  synchronize:
    src: files/notebooks/
    dest: "{{ notebooks_tmp_folder }}/notebooks"

- name: place answers
  synchronize:
    src: files/answers/
    dest: "{{ notebooks_tmp_folder }}/answers/"

- name: mkdir jupyter for every user
  become: true
  become_user: root
  file: path=/home/{{ item.name }}/.jupyter state=directory owner={{ item.name }} group={{ item.name }} mode=0755
  with_items: "{{ users }}"

- name: place jupyter file for every user
  become: true
  become_user: root
  template: src=jupyter_notebook_config.py dest=/home/{{ item.name }}/.jupyter owner={{ item.name }}
  with_items: "{{ users }}"

- name: mkdirs for every user
  become: true
  become_user: root
  file: path={{notebooks_folder}}/{{ item.name }} state=directory owner={{ item.name }} group={{ item.name }} mode=0777
  with_items: "{{ users }}"

- name: copy notebooks and answers for every user
  become: true
  become_user: root
  shell: /bin/cp -R {{ notebooks_tmp_folder }}/* {{ notebooks_folder }}/{{ item.name }}/
  with_items: "{{ users }}"

- name: set ownership for notebooks
  become: true
  become_user: root
  shell: /bin/chown -R {{ item.name }}:{{ item.name }} {{ notebooks_folder }}/{{ item.name }}
  with_items: "{{ users }}"

- name: Copy environment specs
  template: src=environment.yml dest=/tmp/{{ item.name }}-environment.yml mode=755
  with_items: "{{ users }}"

- name: create conda env dir dir
  become: true
  become_user: root
  file: 
    path: "/anaconda/envs/{{ item.env }}"
    state: directory
    mode: 0777
  with_items: "{{ users }}"

- name: set accessability for conda envs
  become: true
  become_user: root
  shell: /bin/chmod -R 777 /anaconda

- name: Update conda
  become: true
  become_user: root
  shell: "/anaconda/bin/conda update conda --quiet -y"

- name: Create virtual conda environment
  become: true
  become_user: root
  shell: "SPARK_HOME={{ spark_home }} /anaconda/bin/conda env create -f /tmp/{{ item.name }}-environment.yml -p /anaconda/envs/{{ item.env }}"
  args:
    creates: "/anaconda/envs/{{ item.env }}/bin"
  with_items: "{{ users }}"

- name: set ownership for conda envs
  become: true
  become_user: root
  shell: /bin/chown -R {{ item.name }}:{{ item.name }} /anaconda/envs/{{ item.env }}
  with_items: "{{ users }}"

- name: Get dataset
  unarchive:
    src: https://storage.googleapis.com/gdd-trainings-bucket/ds-spark-data.zip
    remote_src: yes
    dest: "{{ notebooks_folder }}/data"
    creates: "{{ notebooks_folder }}/data/heroes.csv"

- name: Copy dataset to HDFS too
  become: true
  become_user: root
  shell: hdfs dfs -put "{{ notebooks_folder }}/data" /tmp/

- name: Copy jupyter start script
  template: 
    src: start-jupyter.sh
    dest: "/anaconda/envs/{{ item.env }}"
    mode: 0775
  with_items: "{{ users }}"

- name: Stop jupyter
  shell: pkill jupyter
  ignore_errors: yes

- name: Start jupyter
  become: no
  shell: "nohup sudo -u {{item.name }} /anaconda/envs/{{ item.env }}/start-jupyter.sh >> /tmp/{{ item.name }}-jupyter.log &"
  with_items: "{{ users }}"

- name: Check that Spark conf is present
  stat:
    path: /usr/lib/spark/conf
  register: spark_conf

- name: place spark defaults
  become: true
  become_user: root
  template: src=spark-defaults.conf dest=/tmp/spark-defaults.conf
  when: spark_conf.stat.exists == True

- name: get spark default contents
  become: true
  become_user: root
  shell: cat /tmp/spark-defaults.conf
  register: data
  when: spark_conf.stat.exists == True


- name: insert/update configuration using a local file
  become: true
  become_user: root
  blockinfile:
    dest: "/usr/lib/spark/conf/spark-defaults.conf"
    block: |
      {{ data.stdout }}
  when: spark_conf.stat.exists == True

- name: replace number of executors
  become: true
  become_user: root
  lineinfile:
     dest: "/usr/lib/spark/conf/spark-defaults.conf"
     regexp: 'spark\.executor\.instances'
     line: 'spark.executor.instances 2'
  when: spark_conf.stat.exists == True

- name: replace number of dynamic executors
  become: true
  become_user: root
  lineinfile:
     dest: "/usr/lib/spark/conf/spark-defaults.conf"
     regexp: 'dynamicAllocation\.maxExecutors'
     line: 'spark.dynamicAllocation.maxExecutors 2'
  when: spark_conf.stat.exists == True

- name: replace number of dynamic executors
  become: true
  become_user: root
  lineinfile:
     dest: "/usr/lib/spark/conf/spark-defaults.conf"
     regexp: 'port\.maxRetries'
     line: 'spark.port.maxRetries 200'
  when: spark_conf.stat.exists == True

- name: set SPARK_HOME
  become: true
  become_user: root
  lineinfile: dest=/etc/profile regexp="^export SPARK_HOME=" line="export SPARK_HOME=/usr/lib/spark"
  when: spark_conf.stat.exists == True
